{"meta":{"title":"Hexo","subtitle":"","description":"dgsdgs","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"所有标签","date":"2022-11-12T08:27:04.569Z","updated":"2022-11-12T08:27:04.569Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"","date":"2022-11-12T08:34:30.549Z","updated":"2022-11-12T08:34:30.549Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""},{"title":"我的朋友们","date":"2022-11-12T08:35:45.559Z","updated":"2022-11-12T08:35:45.559Z","comments":true,"path":"friends/index.html","permalink":"http://example.com/friends/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2022-11-12T08:35:11.267Z","updated":"2022-11-12T08:35:11.267Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Hbase_Phoenix学习笔记","slug":"Hbase-Phoenix学习笔记","date":"2022-11-13T07:44:13.000Z","updated":"2022-11-13T07:48:53.397Z","comments":true,"path":"2022/11/13/Hbase-Phoenix学习笔记/","link":"","permalink":"http://example.com/2022/11/13/Hbase-Phoenix%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"I、HBase一、Hbase简介（一）Hbase定义Hbase是分布式的，可扩展的，可存储大量数据的非关系型数据库 （二）逻辑结构 rowkey region 相当于表 列族 列 store:就是一个文件，可以这样理解，一张表行和列族太多了，所以给他切分了，分成了一个一个store,也可以类比于，mysql中表的切分的概念。 （三）物理储存结构 （四）数据模型1.namespace:类似于库 2.region：类似于表 3.Row：HBase 表中的每行数据都由一个 RowKey 和多个 Column（列）组成，数据是按照 RowKey 的字典顺序存储的，并且查询数据时只能根据 RowKey 进行检索，所以 RowKey 的设计十分重要。 4.Column: HBase 中的每个列都由 Column Family(列族)和 Column Qualifier（列限定符）进行限定，例如 info：name，info：age。建表时，只需指明列族，而列限定符无需预先定义。 5.TimeStampe:用于标识数据的不同版本（version），每条数据写入时，如果不指定时间戳，系统会自动为其加上该字段，其值为写入 HBase 的时间。 6.cell:由{rowkey, column Family：column Qualifier, time Stamp} 唯一确定的单元。cell 中的数据是没有类型的，全部是字节码形式存贮。 （五）系统架构 zookeeper Master RegeionServer HdFS Master:管理RegionServer，分配region 到哪个 RegionServer 对于表的操作：create delete alter RegionServer：管理Region，一个RegionServer就是一个集群，里面有很多region（也就是有很多表） 对于数据操作：put get delete 对于Region操作： splitRegion, compactRegion 二、Hbase安装与基本命令1.安装（1）启动命令 bin&#x2F;hbase-start.sh （2）http:hadoop102:16010是web查看的端口号 8080是文件传输的端口号 这一点类似于hdfs 9870wed端口号 8020 是文件传输的端口号 （3）bin&#x2F;hbase shell 基本的命令 （4）hbase 里默认就有hbase system default 这三个namespace,其中hbase中存储的有meta表 2.基本命令 三、Hbase进阶（一）架构原理 （二）写流程1.流程 写：① Client 先访问 zookeeper，找到 Meta 表，并获取 Meta 表元数据。 ② 确定当前将要写入的数据所对应的 HRegion 和 HRegionServer 服务器③ Client 向该 HRegionServer 服务器发起写入数据请求，然后 HRegionServer 收到请求并响应。④ Client 先把数据写入到 HLog，以防止数据丢失。⑤ 然后将数据写入到 Memstore。⑥ 如果 HLog 和 Memstore 均写入成功，则这条数据写入成功⑦ 如果 Memstore 达到阈值，会把 Memstore 中的数据 flush 到 Storefile 中。⑧ 当 Storefile 越来越多，会触发 Compact 合并操作，把过多的 Storefile 合并成一个大的 Storefile。⑨ 当 Storefile 越来越大，Region 也会越来越大，达到阈值后，会触发 Split 操作，将Region 一分为二。 2.注意点（1）在写流程里，有一个知识点是，写入的数据，先是写入到了WAL，但是并没有同步，然后再将数据写入内存，再同步数据，这是一个事务操作，如果没有同步成功，则将会回滚 （2） flush 刷写到磁盘，有一定的规则，就是和时间 和 大小有关， 超过一定时间，或大小就合并了 （3）compact 合并这里有一个知识点，超过一定的时间，他会自动合并，也就是7天，会自动合并，我们一般直接给设置为0，也就是不希望他在一定的时间自动合并超过一定的数量，他也会自动合并，这个数量就是3个，但是，他不会立马就合并了，要等一会，大约二三个小时，我们这里直接手动合并，我们使用compact ‘student’这个命令，直接就将其合并了，然后变为了一个 （三）读流程 读：① HRegionServer 保存着 meta 表以及表数据，要访问表数据，首先 Client 先去访问zookeeper，从 zookeeper 里面获取 meta 表所在的位置信息，即找到这个 meta 表在哪个HRegionServer 上保存着。② 接着 Client 通过刚才获取到的 HRegionServer 的 IP 来访问 Meta 表所在的HRegionServer，从而读取到 Meta，进而获取到 Meta 表中存放的元数据。③ Client 通过元数据中存储的信息，访问对应的 HRegionServer，然后扫描Block Cache Memstore 和 Storefile 来查询数据。然后将查询到的数据合并 此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put&#x2F;Delete）。④ 将从文件中查询到的数据块（Block，HFile 数据存储单元，默认大小为 64KB）缓存到Block Cache。最后 HRegionServer 把查询到的数据响应给 Client。 （四）数据删除flush 和 compact的时候，会删除数据 1.flush假设数据都在内存里，我们put 了好几条数据，rowkey 一样，但是时间戳不一样，然后我们 flush ，只会flush 一条数据， flush 只针对内存里的数据， 2.compact直接将磁盘中的数据删了 3.delete这里有一个很有意思的问题，就是同一个rowKey我们 put 3条数据 ，然后delete ，flush 的时候这个delete标记不能删，如果删了这个标记，那么倒数第二条数据不就活过来了吗 四、Hbase API五、Hbase优化（一）rowkey设计（重点）1.设计原则（1）长度原则 （2）散列原则 （3）唯一原则 2.如何设计（1）生成随机数、hash、散列值 （2）字符串反转 II、Phoenix一、Phoenix简介（一）、定义 （二）、特点 （三）、架构 二、Phoenix快速入门（一）安装12345678910111213141516（1）安装bsdtar3sudo yum install -y epel-releasesudo yum install -y bsdtar3（2）上传并解压tar包[atguigu@hadoop101 module]$ tar -zxvf /opt/software/apache-phoenix-5.0.0-HBase-2.0-bin.tar.gz -C /opt/module[atguigu@hadoop101 module]$ mv apache-phoenix-5.0.0-HBase-2.0-bin phoenix（3）复制server包并拷贝到各个节点的hbase/lib（4）复制client包并拷贝到各个节点的hbase/lib（5）配置环境变量sudo vim /etc/config.d/my_env.sh#phoenixexport PHOENIX_HOME=/opt/module/phoenixexport PHOENIX_CLASSPATH=$PHOENIX_HOMEexport PATH=$PATH:$PHOENIX_HOME/bin（6）启动[atguigu@hadoop101 phoenix]$ bin/sqlline.py hadoop102,hadoop103,hadoop104:2181 （二）phoenix shell操作1.创建表这里在phoenix中建表，没有明确指明列族，我服了 12345678910直接指定单个列作为RowKey这里我有一个疑惑，我的name列，属于哪个列族呢CREATE TABLE IF NOT EXISTS student(id VARCHAR primary key,name VARCHAR);指定多个列的联合作为RowKeyCREATE TABLE IF NOT EXISTS us_population (State CHAR(2) NOT NULL,City VARCHAR NOT NULL,Population BIGINTCONSTRAINT my_pk PRIMARY KEY (state, city)); 1234原来还可以这样写，这样就直接可以指定你的列是哪一个列族了，学到后面才知道，我真是服了,看下面这张图片create table test(&quot;id&quot; varchar primary key,&quot;info1&quot;.&quot;name&quot; varchar, &quot;info2&quot;.&quot;address&quot; varchar); 在phoenix中，表名等会自动转换为大写，若要小写，使用双引号，如”student”，但是这里有个恶心的地方就是，下次你用的使用还要加上”student”, 比如，你insert into “student” values(……),这里””不能少 2.其他操作12345678910111213（1）显示所有表!table或 !tables（2）退出命令行!quit（3）插入数据upsert into student values(&#x27;1001&#x27;,&#x27;zhangsan&#x27;);（4）查询记录select * from student;select * from student where id=&#x27;1001&#x27;;（5）删除记录delete from student where id=&#x27;1001&#x27;;（6）删除表delete from student where id=&#x27;1001&#x27;; 3.表的映射 表的关系：默认情况下，直接在HBase中创建的表，通过Phoenix是查看不到的。它需要在Phoenix中进行表的映射。映射方式有两种：视图映射和表映射。 （1）视图映射 Phoenix创建的视图是只读的，所以只能用来做查询，无法通过视图对源数据进行修改等操作。在phoenix中创建关联test表的视图 （2）表映射 I.HBase中不存在表时，可以直接使用create table指令创建需要的表,系统将会自动在Phoenix和HBase中创建person_infomation的表，并会根据指令内的参数对表结构进行初始化。 II当HBase中已经存在表时，可以以类似创建视图的方式创建关联表，只需要将create view改为create table即可。 （三）phoenix JDBC遇到得到坑 1.jdbc连接phoenix，在idea的代码中，hbase-site.xml中要加入一行 Maven导入依赖 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.apache.phoenix&lt;/groupId&gt; &lt;artifactId&gt;phoenixcore &lt;/artifactId&gt; &lt;version&gt;4.14.2-HBase-1.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.lmax&lt;/groupId&gt; &lt;artifactId&gt;disruptor&lt;/artifactId&gt; &lt;version&gt;3.3.6&lt;/version&gt;&lt;/dependency&gt; 三、Phoenix二级索引（一）HBase协处理器(类似于触发器Trigger)我理解的phoenix二级索引，像mysql建立非聚簇索引类似，我们往一张表插入数据了，然后Hbase会自己往另一张表插入数据 利用协处理器，往数据表中插入数据的同时，往索引表中插入了数据","categories":[{"name":"HBase Phoenix","slug":"HBase-Phoenix","permalink":"http://example.com/categories/HBase-Phoenix/"}],"tags":[{"name":"大数据开发组件","slug":"大数据开发组件","permalink":"http://example.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E7%BB%84%E4%BB%B6/"}]},{"title":"Hello World","slug":"hello-world","date":"2022-11-12T05:36:53.357Z","updated":"2022-11-12T05:36:53.357Z","comments":true,"path":"2022/11/12/hello-world/","link":"","permalink":"http://example.com/2022/11/12/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"HBase Phoenix","slug":"HBase-Phoenix","permalink":"http://example.com/categories/HBase-Phoenix/"}],"tags":[{"name":"大数据开发组件","slug":"大数据开发组件","permalink":"http://example.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E7%BB%84%E4%BB%B6/"}]}